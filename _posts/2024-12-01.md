---
title: 'Micro vs Macro Average'
date: 2024-12-01
permalink: /posts/2024/1/
tags:
  - technical
---
Micro vs Macro Average

Recently, I came across a [paper](https://www.nature.com/articles/s41598-017-18564-8) that got me thinking about how we interpret micro-averaging and macro-averaging in machine learning, especially in multi-class problems. The paper used these two approaches to evaluate classifier performance across different diseases, some of which were really rare. It highlighted some key differences between the two, which I thought would be interesting to share.

So, what’s the difference between micro and macro averages? In a nutshell, macro-averaging treats all classes equally, no matter how frequent or rare they are. It calculates metrics (like AUC or F1-score) for each class separately and then averages them, balancing the contributions. On the other hand, micro-averaging focuses on the overall performance by pooling together all the tp/fp/tn/fn, across all classes and calculating the metric in aggregate, like a weighted average.

In the paper, the authors dedicated a section addressing how the micro-averaged ROC curve consistently outperformed the macro-averaged one. They remarked that this was because 1. the classifiers were doing better on the more prevalent diseases 2. rare diseases dragged down the macro-average 3. the models weren’t handling those well

This is an important takeaway! If you’re working on a problem where rare cases are **just as critical** as common ones , macro-averaging can be a better reflection of your model’s weaknesses. But if the focus is on serving the majority of cases well, optimizing for the micro-averaging will give the desired performance.

The paper reminded me the value of understanding the metrics we report and interpreting them in synch; let us think about what they really mean! It’s not just about which one is "better" but about what question you’re trying to answer. Are you prioritizing equity, or are you focused on efficiency for the majority? These nuances can make a big difference when evaluating models, especially in sensitive applications like healthcare. 