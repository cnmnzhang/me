---
title: 'Harnessing Gaussian Processes for Personalized Modeling in My Research'
date: 2024-12-23
permalink: /posts/2024/12/gp/
tags:
  - technical
---

In my current research with deriving personalized models, we utilize a probabilistic Bayesian inferencing framework—starting with a belief, and updating our beliefs as we receive empirical data, perhaps with a level of adaptability that we learn from another experience. The probabilistic framework allows us to make statements with a level of uncertainty that is meaningful in this continual sense, rather than the unintuitive traditional frequentist sense.

To generate this probability space and make point estimates, one method we explored utilizes Monte Carlo-esque repeated sampling, empowered by the central limit theorem.

> Central Limit Theorem: Given a large enough sample size, the sampling distribution of the sample mean will approximate a normal distribution, regardless of the population's original distribution.

Gaussian processes are benefit from properties inherited from the normal distribution. For example, if a random process is modelled as a Gaussian process, the distributions of various derived quantities such as the mean and standard deviation can be obtained explicitly. GPs enable modeling distributions over functions, providing a flexible means to encode and update prior beliefs with data. By establishing a prior and refining it into a posterior with evidence, GPs facilitate predictions while quantifying uncertainty. This is particularly beneficial in personalized modeling, where individual variability and sparse data are frequent challenges.

Further, while exact models often scale poorly as the amount of data increases, multiple approximation methods have been developed which often retain good accuracy while drastically reducing computation time!

In our work, estimating personalized metrics involves jointly optimizing over two critical parameters: the mean (μ) and the spread (σ) of the data. These parameters are interdependent—changes in one invariably influence the other—necessitating a simultaneous optimization approach. To fit hyperparameters over millions of patients, simulating a model as a Gaussian process allows us to efficiently manage computational complexity while maintaining the ability to capture intricate patterns in patient data.

Some challenges are leveraging these methods as new data points come in. These methods apply well for time series data conceptually for analyzing the updating of beliefs, but the computational prowess still struggles with looping over time series data. Admittedly, I may be doing my analysis naively, but I'm excited to see the findings pertaining to hyper parameter and model stability. 

Overall, I've found that the ability of GP to provide uncertainty estimates while balanceing robustness with adaptability makes them an invaluable tool in bridging the gap between data and actionable insights. I'm excited to see how we can quantify adaptability and the possible associations with health outcomes! It's just so cool to see this mathematical processes come handy in real life applications. 

